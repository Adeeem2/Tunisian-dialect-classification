{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T11:58:02.380609900Z",
     "start_time": "2025-12-23T11:57:43.858479700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"arbml/Tunisian_Dialect_Corpus\")\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df.to_csv(\"tunisian_dialect_corpus.csv\", index=False)\n"
   ],
   "id": "21ceca5fa53cf26e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T11:58:04.133019700Z",
     "start_time": "2025-12-23T11:58:04.074492300Z"
    }
   },
   "cell_type": "code",
   "source": "df\n",
   "id": "7568bb472986c8a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   Tweet  label\n",
       "0                                   Nn mouch 7louwa faza      1\n",
       "1                                mabladkom 3bed tfouuuuh      1\n",
       "2      تفووووووه عليك و علا والديك على عايلتك و على ا...      1\n",
       "3                                  لا يليق بهذا البرنامج      1\n",
       "4                                                  رهدان      1\n",
       "...                                                  ...    ...\n",
       "49884                                        الله يستر ن      0\n",
       "49885                                        الله يستر ن      0\n",
       "49886  ربي اكون فى عونكم بالحق ربي ابقي الستر على تون...      0\n",
       "49887                                                         0\n",
       "49888                                      ربي يلطف بينا      0\n",
       "\n",
       "[49889 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nn mouch 7louwa faza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mabladkom 3bed tfouuuuh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تفووووووه عليك و علا والديك على عايلتك و على ا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لا يليق بهذا البرنامج</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>رهدان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49884</th>\n",
       "      <td>الله يستر ن</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49885</th>\n",
       "      <td>الله يستر ن</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49886</th>\n",
       "      <td>ربي اكون فى عونكم بالحق ربي ابقي الستر على تون...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49887</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49888</th>\n",
       "      <td>ربي يلطف بينا</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49889 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T11:58:11.387230400Z",
     "start_time": "2025-12-23T11:58:09.094628200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# One preprocessing function (keep meaning, reduce social-media noise)\n",
    "# Goals:\n",
    "# - remove only low-value noise (URLs, mentions, ZWJ, tatweel, extra spaces)\n",
    "# - normalize Arabic variants without changing meaning\n",
    "# - reduce elongation (ههههه, loooool) but keep the word\n",
    "# - handle Tunisian Arabizi digits (7=ح, 3=ع...) conservatively\n",
    "# - keep emojis and punctuation (they can carry sentiment)\n",
    "\n",
    "_ARABIC_DIACRITICS_RE = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
    "_TATWEEL_RE = re.compile(r\"\\u0640\")\n",
    "_URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "_MENTION_RE = re.compile(r\"@\\w+\")\n",
    "\n",
    "# collapse 3+ repeats -> 2 (works for Arabic + Latin)\n",
    "_REPEAT_CHARS_RE = re.compile(r\"(.)\\1{2,}\")\n",
    "\n",
    "# collapse repeated punctuation !!!! -> !\n",
    "_REPEAT_PUNCT_RE = re.compile(r\"([!?.,؛،])\\1{1,}\")\n",
    "\n",
    "# keep: Arabic letters, Latin letters, digits, whitespace, common punctuation, emojis\n",
    "# We'll *remove* only control chars and rare symbols later.\n",
    "_CONTROL_CHARS_RE = re.compile(r\"[\\u200b\\u200c\\u200d\\ufeff]\")  # ZWSP/ZWNJ/ZWJ/BOM\n",
    "\n",
    "_ARABIZI_DIGIT_MAP = str.maketrans(\n",
    "    {\n",
    "        \"2\": \"ء\",\n",
    "        \"3\": \"ع\",\n",
    "        \"4\": \"غ\",  # sometimes used\n",
    "        \"5\": \"خ\",\n",
    "        \"6\": \"ط\",\n",
    "        \"7\": \"ح\",\n",
    "        \"8\": \"ق\",\n",
    "        \"9\": \"ق\",\n",
    "        \"0\": \"0\",\n",
    "    }\n",
    ")\n",
    "\n",
    "_ARABIC_INDIC_DIGITS = str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\")\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    # remove URLs + mentions (keep hashtags content)\n",
    "    text = _URL_RE.sub(\" \", text)\n",
    "    text = _MENTION_RE.sub(\" \", text)\n",
    "    text = text.replace(\"#\", \"\")\n",
    "\n",
    "    # remove invisible control chars (common in copy/pasted tweets)\n",
    "    text = _CONTROL_CHARS_RE.sub(\"\", text)\n",
    "\n",
    "    # normalize unicode form\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # remove Arabic diacritics + tatweel\n",
    "    text = _ARABIC_DIACRITICS_RE.sub(\"\", text)\n",
    "    text = _TATWEEL_RE.sub(\"\", text)\n",
    "\n",
    "    # normalize Arabic variants (low-risk)\n",
    "    text = re.sub(r\"[أإآ]\", \"ا\", text)\n",
    "    text = text.replace(\"ى\", \"ي\")\n",
    "    text = text.replace(\"ؤ\", \"و\")\n",
    "    text = text.replace(\"ئ\", \"ي\")\n",
    "\n",
    "    # normalize Arabic punctuation variants\n",
    "    text = text.replace(\"؟\", \"?\").replace(\"،\", \",\").replace(\"؛\", \";\")\n",
    "\n",
    "    # convert Arabic-Indic digits -> Western\n",
    "    text = text.translate(_ARABIC_INDIC_DIGITS)\n",
    "\n",
    "    # (Removed simple Arabizi mapping to allow advanced token-level normalization later)\n",
    "\n",
    "    # lower ONLY latin letters (keep Arabic as-is)\n",
    "    # this is safer than .lower() on the whole string for some unicode edge cases\n",
    "    text = \"\".join(ch.lower() if \"A\" <= ch <= \"Z\" else ch for ch in text)\n",
    "\n",
    "    # reduce elongations (3+ repeats -> 2)\n",
    "    text = _REPEAT_CHARS_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # reduce repeated punctuation\n",
    "    text = _REPEAT_PUNCT_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    # remove leftover junk symbols but keep emojis:\n",
    "    # keep Arabic letters, Latin letters, digits, spaces, and a small punctuation set.\n",
    "    # Anything else becomes a space.\n",
    "    text = re.sub(r\"[^ء-يA-Za-z0-9\\s!?.,;:'\\\"()\\[\\]{}<>+-=_/\\\\]\", \" \", text)\n",
    "\n",
    "    # collapse spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# keep raw Tweet; write cleaned text in a new column\n",
    "df[\"text\"] = df[\"Tweet\"].apply(preprocess_text)\n",
    "\n",
    "# drop empty after preprocessing\n",
    "df = df[df[\"text\"].str.strip().ne(\"\")].reset_index(drop=True)\n",
    "\n",
    "df[[\"Tweet\", \"text\", \"label\"]].head()\n"
   ],
   "id": "b6c5dec17f538594",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               Tweet  \\\n",
       "0                               Nn mouch 7louwa faza   \n",
       "1                            mabladkom 3bed tfouuuuh   \n",
       "2  تفووووووه عليك و علا والديك على عايلتك و على ا...   \n",
       "3                              لا يليق بهذا البرنامج   \n",
       "4                                              رهدان   \n",
       "\n",
       "                                                text  label  \n",
       "0                               nn mouch 7louwa faza      1  \n",
       "1                              mabladkom 3bed tfouuh      1  \n",
       "2  تفووه عليك و علا والديك علي عايلتك و علي اصلك ...      1  \n",
       "3                              لا يليق بهذا البرنامج      1  \n",
       "4                                              رهدان      1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nn mouch 7louwa faza</td>\n",
       "      <td>nn mouch 7louwa faza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mabladkom 3bed tfouuuuh</td>\n",
       "      <td>mabladkom 3bed tfouuh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تفووووووه عليك و علا والديك على عايلتك و على ا...</td>\n",
       "      <td>تفووه عليك و علا والديك علي عايلتك و علي اصلك ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لا يليق بهذا البرنامج</td>\n",
       "      <td>لا يليق بهذا البرنامج</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>رهدان</td>\n",
       "      <td>رهدان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T11:58:20.797798600Z",
     "start_time": "2025-12-23T11:58:19.269488600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================================================\n",
    "# TEST: Translation using Dictionary from \"Translation arabizi to arabic.ipynb\"\n",
    "# ========================================================================\n",
    "\n",
    "# Dictionary from the translation file\n",
    "buck2uni = { \"e\":\"ا\",\n",
    "            \"é\":\"ا\",\n",
    "            \"a\":\"ا\",\n",
    "\n",
    "     \"7\":\"ح\",\n",
    "             \"7a\": \"ح\",\n",
    "        \"7e\": \"ح\",\n",
    "        \"7i\": \"ح\",\n",
    "        \"7o\": \"ح\",\n",
    "        \"7u\": \"ح\",\n",
    "            \"5\":\"خ\",\n",
    "            \"3\":\"ع\",\n",
    "     \"3\":\"ع\",\n",
    "            \"9\":\"ق\",\n",
    "    \"9\":\"ق\",\n",
    "    \"8\":\"غ\",\n",
    "            \"3a\": \"ع\",\n",
    "        \"3e\": \"ع\",\n",
    "        \"3i\":\"ع\",\n",
    "        \"3o\": \"ع\",\n",
    "        \"3u\": \"ع\",\n",
    "        \"5a\": \"خ\",\n",
    "        \"5e\": \"خ\",\n",
    "        \"5i\": \"خ\",\n",
    "        \"5o\": \"خ\",\n",
    "        \"5u\": \"خ\",\n",
    "            \"8\":\"غ\",\n",
    "            \"2\": \"ا\",\n",
    "   \"a\": \"ا\",\n",
    "\n",
    "        \"b\": \"ب\",\n",
    "        \"ba\": \"ب\",\n",
    "        \"be\": \"ب\",\n",
    "        \"bi\": \"ب\",\n",
    "        \"bo\": \"ب\",\n",
    "        \"bu\": \"ب\",\n",
    "        \"ch\": \"ش\",\n",
    "        \"cha\": \"ش\",\n",
    "        \"che\":\"ش\",\n",
    "        \"chi\": \"ش\",\n",
    "        \"cho\": \"ش\",\n",
    "        \"chu\": \"ش\",\n",
    "\n",
    "        \"b\": \"ب\",\n",
    "\n",
    "        \"ch\": \"ش\",\n",
    "\n",
    "        \"d\": \"د\",\n",
    "\n",
    "       \"c\" : \"ك\",\n",
    "\n",
    "       \"ai\": \"ي\",\n",
    "\n",
    "        \"ou\" : \"و\",\n",
    "        \"th\": \"ذ\",\n",
    "             \"tha\": \"ذ\",\n",
    "             \"tha\": \"ث\",\n",
    "            \"the\":\"ذ\",\n",
    "             \"the\":  \"ث\",\n",
    "             \"the\":\"ذ\",\n",
    "             \"tho\":   \"ث\",\n",
    "             \"tho\":\"ذ\",\n",
    "            \"thi\":  \"ث\",\n",
    "            \"thi\":\"ذ\",\n",
    "             \"the\":  \"ث\",\n",
    "        \"dh\":  \"ظ\",\n",
    "       \"dh\"  : \"ض\",\n",
    "         \"dha\":  \"ظ\",\n",
    "       \"dha\"  : \"ض\",\n",
    "              \"dhe\":  \"ظ\",\n",
    "       \"dhe\"  : \"ض\",\n",
    "\n",
    "                          \"dhe\":  \"ظ\",\n",
    "       \"dhe\"  : \"ض\",\n",
    "\n",
    "\n",
    "            \"f\": \"ف\",\n",
    "\"r\":  \"ر\",\n",
    "        \"ra\": \"ر\",\n",
    "        \"re\": \"ر\",\n",
    "        \"ri\": \"ر\",\n",
    "        \"ro\": \"ر\",\n",
    "        \"ru\": \"ر\",\n",
    "        \"fa\": \"ف\",\n",
    "        \"fe\": \"ف\",\n",
    "        \"fi\": \"ف\",\n",
    "        \"fo\": \"ف\",\n",
    "        \"fu\": \"ف\",\n",
    "        \"gh\": \"غ\",\n",
    " \"k\": \"ك\",\n",
    "        \"ka\": \"ك\",\n",
    "        \"ke\": \"ك\",\n",
    "        \"ki\":\"ك\",\n",
    "        \"ko\": \"ك\",\n",
    "        \"ku\":\"ك\",\n",
    "        \"kh\":  \"خ\",\n",
    "        \"kha\": \"خ\",\n",
    "        \"khe\": \"خ\",\n",
    "        \"khi\": \"خ\",\n",
    "        \"kho\": \"خ\",\n",
    "        \"khu\": \"خ\",\n",
    "        \"h\": \"ه\",\n",
    "        \"ha\": \"ه\",\n",
    "         \"gh\": \"غ\",\n",
    "        \"gha\": \"غ\",\n",
    "        \"ghe\": \"غ\",\n",
    "        \"ghi\": \"غ\",\n",
    "        \"gho\":\"غ\",\n",
    "        \"ghu\": \"غ\",\n",
    "        \"h\": \"ه\",\n",
    "        \"ha\": \"ه\",\n",
    "        \"he\": \"ه\",\n",
    "        \"hi\": \"ه\",\n",
    "        \"ho\": \"ه\",\n",
    "        \"hu\": \"ه\",\n",
    "        \"i\": \"ى\",\n",
    "                    \"i\": \"ي\",\n",
    "\n",
    "        \"ia\": \"ي\",\n",
    "        \"ie\": \"ي\",\n",
    "        \"ii\": \"ي\",\n",
    "        \"io\": \"ي\",\n",
    "        \"iu\": \"ي\",\n",
    "        \"i\": \"ي\",\n",
    "        \"j\": \"ج\",\n",
    "\n",
    "        \"k\": \"ك\",\n",
    "        \"ka\": \"\",\n",
    "\n",
    "        \"kh\":  \"خ\",\n",
    "        \"ch\":\"ش\",\n",
    "\n",
    "        \"l\":  \"ل\",\n",
    "        \"l\": \"ل\",\n",
    "\n",
    "        \"m\":  \"م\",\n",
    "\n",
    "        \"n\":  \"ن\",\n",
    "\n",
    "        \"o\":  \"و\",\n",
    "\n",
    "        \"ou\": \"و\",\n",
    "\n",
    "        \"p\":  \"ب\",\n",
    "\n",
    "\n",
    "        \"q\":  \"ك\",\n",
    "\n",
    "        \"r\":  \"ر\" ,\n",
    "        \"ra\": \"ر\",\n",
    "\n",
    "        \"s\":  \"س\",\n",
    "\n",
    "     \"ch\": \"ش\",\n",
    "        \"sh\": \"ش\",\n",
    "        \"t\":  \"ت\",\n",
    "\n",
    "     \"t\":  \"ط\",\n",
    "        \"ti\": \"ت\",\n",
    "               \"ti\":  \"ط\",\n",
    "        \"to\": \"ت\",\n",
    "             \"to\":  \"ط\",\n",
    "        \"tu\": \"ت\",\n",
    "             \"tu\":  \"ط\",\n",
    "        \"ta\":  \"ط\",\n",
    "\n",
    "        \"ta\": \"ت\",\n",
    "        \"te\": \"ت\",\n",
    "            \"te\": \"ط\",\n",
    "\n",
    "        \"th\":  \"ث\" ,\n",
    "        \"th\":  \"ذ\",\n",
    "\n",
    "     \"t\": \"ت\",\n",
    "         \"t\": \"ط\",\n",
    "        \"w\": \"و\",\n",
    "        \"g\": \"ق\",\n",
    "        \"ga\": \"ق\",\n",
    "        \"ge\": \"ج\",\n",
    "        \"y\": \"ي\",\n",
    "\n",
    "        \"v\" :\"ف\",\n",
    "         \"ph\" :\"ف\",\n",
    "        \"z\":  \"ز\",\n",
    "                   \"l\":  \"ل\",\n",
    "        \"la\": \"ل\",\n",
    "        \"le\": \"ل\",\n",
    "        \"li\": \"ل\",\n",
    "        \"lo\": \"ل\",\n",
    "        \"lu\": \"ل\",\n",
    "        \"m\":  \"م\",\n",
    "        \"ma\": \"م\",\n",
    "        \"me\": \"م\",\n",
    "        \"mi\": \"م\",\n",
    "        \"mo\": \"م\",\n",
    "        \"mu\": \"م\",\n",
    "        \"n\":  \"ن\",\n",
    "        \"na\": \"ن\",\n",
    "        \"ne\": \"ن\",\n",
    "        \"ni\": \"ن\",\n",
    "        \"no\": \"ن\",\n",
    "        \"nu\": \"ن\",\n",
    "        \"o\":  \"ا\",\n",
    "        \"p\":  \"ب\",\n",
    "        \"pa\": \"ب\",\n",
    "        \"pe\": \"ب\",\n",
    "        \"pi\": \"ب\",\n",
    "        \"po\": \"ب\",\n",
    "        \"pu\": \"ب\",\n",
    "             \"u\":  \"و\",\n",
    "        \"ua\": \"و\",\n",
    "        \"ue\": \"و\",\n",
    "        \"ui\": \"و\",\n",
    "        \"uo\": \"و\",\n",
    "        \"uu\": \"و\",\n",
    "        \"w\":  \"و\",\n",
    "        \"wa\": \"و\",\n",
    "        \"we\": \"و\",\n",
    "        \"wi\": \"و\",\n",
    "        \"wo\": \"و\",\n",
    "        \"wu\": \"و\",\n",
    "            \"y\": \"ي\",\n",
    "            \"y\": \"ى\",\n",
    "        \"ya\": \"ي\",\n",
    "        \"ye\": \"ي\",\n",
    "        \"yi\": \"ي\",\n",
    "\n",
    " }\n",
    "\n",
    "\n",
    "def transString1(string, reverse=0):\n",
    "    '''Given a Unicode string, transliterate into Buckwalter. To go from\n",
    "    Buckwalter back to Unicode, set reverse=1'''\n",
    "\n",
    "    if not isinstance(string, str):\n",
    "        return \"\"\n",
    "\n",
    "    for k, v in buck2uni.items():\n",
    "        if not reverse:\n",
    "            string = string.replace(v, k)\n",
    "        else:\n",
    "            string = string.replace(k, v)\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "# Test on a sample of 20 rows\n",
    "print(\"Testing Dictionary-Based Translation on Sample Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply translation (reverse=1 means Arabizi to Arabic)\n",
    "df['translated_dict'] = df['text'].astype(str).str.lower().apply(lambda x: transString1(x, reverse=1))\n",
    "\n",
    "# Display results\n",
    "comparison_df = df[['text', 'translated_dict']].copy()\n",
    "\n",
    "print(\"\\nOriginal vs Dictionary Translation:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Display as dataframe\n",
    "print(\"\\n\\nComparison Table:\")\n",
    "comparison_df\n"
   ],
   "id": "651a8d99825aa5ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Dictionary-Based Translation on Sample Data\n",
      "================================================================================\n",
      "\n",
      "Original vs Dictionary Translation:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Comparison Table:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                    text  \\\n",
       "0                                   nn mouch 7louwa faza   \n",
       "1                                  mabladkom 3bed tfouuh   \n",
       "2      تفووه عليك و علا والديك علي عايلتك و علي اصلك ...   \n",
       "3                                  لا يليق بهذا البرنامج   \n",
       "4                                                  رهدان   \n",
       "...                                                  ...   \n",
       "47790                                  يااحسرة اليوم 200   \n",
       "47791                                        الله يستر ن   \n",
       "47792                                        الله يستر ن   \n",
       "47793  ربي اكون في عونكم بالحق ربي ابقي الستر علي تون...   \n",
       "47794                                      ربي يلطف بينا   \n",
       "\n",
       "                                         translated_dict  \n",
       "0                                      نن موش حلووا فازا  \n",
       "1                                   مابلادكام عباد طفووه  \n",
       "2      تفووه عليك و علا والديك علي عايلتك و علي اصلك ...  \n",
       "3                                  لا يليق بهذا البرنامج  \n",
       "4                                                  رهدان  \n",
       "...                                                  ...  \n",
       "47790                                  يااحسرة اليوم ا00  \n",
       "47791                                        الله يستر ن  \n",
       "47792                                        الله يستر ن  \n",
       "47793  ربي اكون في عونكم بالحق ربي ابقي الستر علي تون...  \n",
       "47794                                      ربي يلطف بينا  \n",
       "\n",
       "[47795 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>translated_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nn mouch 7louwa faza</td>\n",
       "      <td>نن موش حلووا فازا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mabladkom 3bed tfouuh</td>\n",
       "      <td>مابلادكام عباد طفووه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تفووه عليك و علا والديك علي عايلتك و علي اصلك ...</td>\n",
       "      <td>تفووه عليك و علا والديك علي عايلتك و علي اصلك ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لا يليق بهذا البرنامج</td>\n",
       "      <td>لا يليق بهذا البرنامج</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>رهدان</td>\n",
       "      <td>رهدان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47790</th>\n",
       "      <td>يااحسرة اليوم 200</td>\n",
       "      <td>يااحسرة اليوم ا00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47791</th>\n",
       "      <td>الله يستر ن</td>\n",
       "      <td>الله يستر ن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47792</th>\n",
       "      <td>الله يستر ن</td>\n",
       "      <td>الله يستر ن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47793</th>\n",
       "      <td>ربي اكون في عونكم بالحق ربي ابقي الستر علي تون...</td>\n",
       "      <td>ربي اكون في عونكم بالحق ربي ابقي الستر علي تون...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47794</th>\n",
       "      <td>ربي يلطف بينا</td>\n",
       "      <td>ربي يلطف بينا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47795 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T11:58:54.595916800Z",
     "start_time": "2025-12-23T11:58:53.935051100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------# IMPROVED CODA-Compliant Tunisian Arabizi -> Arabic Transliteration System\n",
    "# ---------------------------------------------------------\n",
    "# Following Conventional Orthography for Dialectal Arabic (CODA)\n",
    "# Enhanced with dictionary mappings from the translation file\n",
    "# Handles mixed languages (Tunisian Arabizi, French, English)\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "class CODATunisianTransliterator:\n",
    "    \"\"\"\n",
    "    Improved CODA-compliant Tunisian Arabizi to Arabic transliterator.\n",
    "    Uses dictionary mappings from translation file with context-aware rules.\n",
    "    Handles mixed content: Tunisian Arabizi, French, and English.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_list=None):\n",
    "        self.vocab = set(vocab_list) if vocab_list else set()\n",
    "        self.cache = {}\n",
    "\n",
    "        # CODA Exception Lexicon: Common Tunisian expressions and abbreviations\n",
    "        self.exception_lexicon = {\n",
    "            'nchal': 'إن شاء الله',\n",
    "            'nchalah': 'إن شاء الله',\n",
    "            'inchallah': 'إن شاء الله',\n",
    "            'inshallah': 'إن شاء الله',\n",
    "            'md': 'الحمد لله',\n",
    "            'hamdoulah': 'الحمد لله',\n",
    "            'hamdoullah': 'الحمد لله',\n",
    "            'mdl': 'الحمد لله',\n",
    "            'slt': 'سلام',\n",
    "            'slm': 'سلام',\n",
    "            'salam': 'سلام',\n",
    "            'cv': 'كيفاش',\n",
    "            'chbi': 'شبي',\n",
    "            'chbeik': 'شبيك',\n",
    "            'chkoun': 'شكون',\n",
    "            'chkun': 'شكون',\n",
    "            'chwaya': 'شوية',\n",
    "            'chwiya': 'شوية',\n",
    "            'barcha': 'برشا',\n",
    "            'behi': 'باهي',\n",
    "            'bahi': 'باهي',\n",
    "            'wa9tach': 'وقتاش',\n",
    "            'wa9tech': 'وقتاش',\n",
    "            'win': 'وين',\n",
    "            'winek': 'وينك',\n",
    "            'kifech': 'كيفاش',\n",
    "            'kifech': 'كيفاش',\n",
    "            'ey': 'إي',\n",
    "            'ay': 'أي',\n",
    "            'yezzi': 'يزي',\n",
    "            'wala': 'ولا',\n",
    "            'walla': 'ولا',\n",
    "            'ama': 'أما',\n",
    "            'yaser': 'ياسر',\n",
    "            'yacer': 'ياسر',\n",
    "            'tounes': 'تونس',\n",
    "            'tounsi': 'تونسي',\n",
    "        }\n",
    "\n",
    "        # Enhanced digit mappings from dictionary (CODA standard + Tunisian variants)\n",
    "        self.digit_map = {\n",
    "            '2': 'ء',\n",
    "            '3': 'ع',\n",
    "            '4': 'غ',\n",
    "            '5': 'خ',\n",
    "            '6': 'ط',\n",
    "            '7': 'ح',\n",
    "            '8': 'غ',\n",
    "            '9': 'ق',\n",
    "            '0': 'و',  # Sometimes used for و\n",
    "        }\n",
    "\n",
    "        # Multi-character patterns from dictionary (order matters - longest first)\n",
    "        # These should be checked before single characters\n",
    "        self.multi_char_patterns = [\n",
    "            # Consonant combinations\n",
    "            ('kha', 'خ'),\n",
    "            ('khe', 'خ'),\n",
    "            ('khi', 'خ'),\n",
    "            ('kho', 'خ'),\n",
    "            ('khu', 'خ'),\n",
    "            ('kh', 'خ'),\n",
    "\n",
    "            ('cha', 'ش'),\n",
    "            ('che', 'ش'),\n",
    "            ('chi', 'ش'),\n",
    "            ('cho', 'ش'),\n",
    "            ('chu', 'ش'),\n",
    "            ('ch', 'ش'),\n",
    "            ('sh', 'ش'),\n",
    "\n",
    "            ('gha', 'غ'),\n",
    "            ('ghe', 'غ'),\n",
    "            ('ghi', 'غ'),\n",
    "            ('gho', 'غ'),\n",
    "            ('ghu', 'غ'),\n",
    "            ('gh', 'غ'),\n",
    "\n",
    "            ('tha', 'ث'),\n",
    "            ('the', 'ث'),\n",
    "            ('thi', 'ث'),\n",
    "            ('tho', 'ث'),\n",
    "            ('thu', 'ث'),\n",
    "            ('th', 'ث'),\n",
    "\n",
    "            ('dha', 'ض'),\n",
    "            ('dhe', 'ض'),\n",
    "            ('dhi', 'ض'),\n",
    "            ('dho', 'ض'),\n",
    "            ('dhu', 'ض'),\n",
    "            ('dh', 'ذ'),\n",
    "\n",
    "            # Digit combinations\n",
    "            ('7a', 'ح'),\n",
    "            ('7e', 'ح'),\n",
    "            ('7i', 'ح'),\n",
    "            ('7o', 'ح'),\n",
    "            ('7u', 'ح'),\n",
    "\n",
    "            ('3a', 'ع'),\n",
    "            ('3e', 'ع'),\n",
    "            ('3i', 'ع'),\n",
    "            ('3o', 'ع'),\n",
    "            ('3u', 'ع'),\n",
    "\n",
    "            ('5a', 'خ'),\n",
    "            ('5e', 'خ'),\n",
    "            ('5i', 'خ'),\n",
    "            ('5o', 'خ'),\n",
    "            ('5u', 'خ'),\n",
    "\n",
    "            # Vowel combinations\n",
    "            ('ou', 'و'),\n",
    "            ('oo', 'و'),\n",
    "            ('ai', 'ي'),\n",
    "            ('ei', 'ي'),\n",
    "            ('aa', 'ا'),\n",
    "            ('ee', 'ي'),\n",
    "            ('ii', 'ي'),\n",
    "            ('uu', 'و'),\n",
    "            ('ph', 'ف'),\n",
    "        ]\n",
    "\n",
    "        # Single character mappings (based on dictionary)\n",
    "        self.char_map = {\n",
    "            'a': 'ا',\n",
    "            'e': 'ا',\n",
    "            'é': 'ا',\n",
    "            'b': 'ب',\n",
    "            'c': 'ك',\n",
    "            'd': 'د',\n",
    "            'f': 'ف',\n",
    "            'g': 'ق',  # or ج depending on context\n",
    "            'h': 'ه',\n",
    "            'i': 'ي',\n",
    "            'j': 'ج',\n",
    "            'k': 'ك',\n",
    "            'l': 'ل',\n",
    "            'm': 'م',\n",
    "            'n': 'ن',\n",
    "            'o': 'و',\n",
    "            'p': 'ب',\n",
    "            'q': 'ك',\n",
    "            'r': 'ر',\n",
    "            's': 'س',\n",
    "            't': 'ت',\n",
    "            'u': 'و',\n",
    "            'v': 'ف',\n",
    "            'w': 'و',\n",
    "            'x': 'كس',\n",
    "            'y': 'ي',\n",
    "            'z': 'ز',\n",
    "        }\n",
    "\n",
    "        # French/English common words to preserve (not translate)\n",
    "        self.foreign_words = {\n",
    "            # French\n",
    "            'ok', 'oui', 'non', 'merci', 'bonjour', 'bonsoir', 'salut',\n",
    "            'bien', 'mal', 'bon', 'mauvais', 'super', 'cool', 'top',\n",
    "            'parce', 'que', 'je', 'tu', 'il', 'elle', 'nous', 'vous',\n",
    "            'tout', 'rien', 'plus', 'moins', 'tres', 'beaucoup',\n",
    "            'comment', 'quoi', 'qui', 'ou', 'quand', 'pourquoi',\n",
    "            'la', 'le', 'les', 'un', 'une', 'des', 'de', 'du',\n",
    "            'et', 'ou', 'mais', 'donc', 'car', 'ni', 'or',\n",
    "            'meme', 'deja', 'encore', 'toujours', 'jamais',\n",
    "            'aujourd', 'hui', 'demain', 'hier',\n",
    "            # English\n",
    "            'yes', 'no', 'okay', 'hi', 'hello', 'bye', 'thanks',\n",
    "            'sorry', 'please', 'good', 'bad', 'nice', 'great',\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',\n",
    "            'to', 'for', 'of', 'with', 'from', 'by',\n",
    "            'this', 'that', 'these', 'those',\n",
    "            'what', 'when', 'where', 'why', 'how', 'who',\n",
    "            'lol', 'omg', 'wtf', 'tbh', 'btw',\n",
    "        }\n",
    "\n",
    "    def preprocess_arabizi(self, text):\n",
    "        \"\"\"\n",
    "        Step 1: Preprocessing according to CODA rules\n",
    "        - Remove repeated letters for emphasis\n",
    "        - Replace emoticons/emojis with #\n",
    "        - Handle CODA markers (+ for joining, - for splitting)\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Replace emoticons and emojis with #\n",
    "        # Basic emoticons\n",
    "        text = re.sub(r'[:;=]-?[\\)\\(DPpOo\\[\\]{}|\\\\\\/]', '#', text)\n",
    "        # Emoji range (basic coverage)\n",
    "        text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', '#', text)\n",
    "\n",
    "        # Remove repeated letters (3+ -> 2 for Arabic, 1 for emphasis)\n",
    "        # More conservative: 3+ same letter -> 2\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def is_foreign_word(self, word):\n",
    "        \"\"\"Check if word is French/English and should be skipped\"\"\"\n",
    "        return word.lower() in self.foreign_words\n",
    "\n",
    "    def handle_coda_markers(self, tokens):\n",
    "        \"\"\"\n",
    "        Handle CODA-specific markers:\n",
    "        - '+' joins words into single Arabic word\n",
    "        - '-' splits into two Arabic words\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "\n",
    "            # Handle joining with '+'\n",
    "            if '+' in token:\n",
    "                parts = token.split('+')\n",
    "                # Transliterate each part then join without space\n",
    "                joined = ''.join([self.transliterate_token(p) for p in parts if p])\n",
    "                result.append(joined)\n",
    "            # Handle splitting with '-'\n",
    "            elif '-' in token:\n",
    "                parts = token.split('-')\n",
    "                # Transliterate each part as separate word\n",
    "                result.extend([self.transliterate_token(p) for p in parts if p])\n",
    "            else:\n",
    "                result.append(token)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def is_arabizi(self, token):\n",
    "        \"\"\"Check if token contains Latin/digits (Arabizi) and no Arabic\"\"\"\n",
    "        if not token:\n",
    "            return False\n",
    "        has_latin = bool(re.search(r'[a-zA-Z0-9]', token))\n",
    "        has_arabic = bool(re.search(r'[ء-ي]', token))\n",
    "        return has_latin and not has_arabic\n",
    "\n",
    "    def transliterate_token(self, token):\n",
    "        \"\"\"\n",
    "        Core transliteration using dictionary-based approach with CODA rules\n",
    "        \"\"\"\n",
    "        if not token or not isinstance(token, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Step 2: Check exception lexicon first\n",
    "        token_lower = token.lower()\n",
    "        if token_lower in self.exception_lexicon:\n",
    "            return self.exception_lexicon[token_lower]\n",
    "\n",
    "        # Skip if foreign word\n",
    "        if self.is_foreign_word(token_lower):\n",
    "            return token\n",
    "\n",
    "        # Skip pure numbers\n",
    "        if token.isdigit():\n",
    "            return token\n",
    "\n",
    "        # If already Arabic, return as-is\n",
    "        if not self.is_arabizi(token):\n",
    "            return token\n",
    "\n",
    "        # Step 3: Apply phonetic rules using dictionary mappings\n",
    "        text = token_lower\n",
    "        result = \"\"\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            matched = False\n",
    "\n",
    "            # Try multi-character patterns first (longest match priority)\n",
    "            for pattern, arabic in self.multi_char_patterns:\n",
    "                if text[i:i+len(pattern)] == pattern:\n",
    "                    result += arabic\n",
    "                    i += len(pattern)\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            if matched:\n",
    "                continue\n",
    "\n",
    "            # Try digit mappings\n",
    "            if text[i] in self.digit_map:\n",
    "                result += self.digit_map[text[i]]\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Single character mapping\n",
    "            char = text[i]\n",
    "            if char in self.char_map:\n",
    "                result += self.char_map[char]\n",
    "                i += 1\n",
    "            else:\n",
    "                # Unknown character (punctuation, etc.), keep as-is\n",
    "                result += char\n",
    "                i += 1\n",
    "\n",
    "        return result if result else token\n",
    "\n",
    "    def get_best_vocab_match(self, transliterated, threshold=0.35):\n",
    "        \"\"\"\n",
    "        Match transliterated word against vocabulary using edit distance\n",
    "        \"\"\"\n",
    "        if not self.vocab or not transliterated:\n",
    "            return transliterated\n",
    "\n",
    "        if transliterated in self.cache:\n",
    "            return self.cache[transliterated]\n",
    "\n",
    "        best_word = transliterated\n",
    "        min_dist = float('inf')\n",
    "\n",
    "        # Filter candidates by length similarity\n",
    "        candidates = [w for w in self.vocab if abs(len(w) - len(transliterated)) <= 2]\n",
    "\n",
    "        for word in candidates:\n",
    "            d = levenshtein_distance(transliterated, word)\n",
    "            norm_d = d / max(len(transliterated), len(word))\n",
    "\n",
    "            if norm_d < min_dist:\n",
    "                min_dist = norm_d\n",
    "                best_word = word\n",
    "\n",
    "        if min_dist <= threshold:\n",
    "            self.cache[transliterated] = best_word\n",
    "            return best_word\n",
    "        else:\n",
    "            self.cache[transliterated] = transliterated\n",
    "            return transliterated\n",
    "\n",
    "    def transliterate_text(self, text, use_vocab_matching=True):\n",
    "        \"\"\"\n",
    "        Full pipeline: preprocess -> transliterate -> vocab match\n",
    "        \"\"\"\n",
    "        # Step 1: Preprocess\n",
    "        text = self.preprocess_arabizi(text)\n",
    "\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Handle CODA markers (+, -)\n",
    "        tokens = self.handle_coda_markers(tokens)\n",
    "\n",
    "        # Transliterate each token\n",
    "        result_tokens = []\n",
    "        for token in tokens:\n",
    "            transliterated = self.transliterate_token(token)\n",
    "\n",
    "            # Optional: match against vocabulary\n",
    "            if use_vocab_matching and self.vocab and self.is_arabizi(token):\n",
    "                transliterated = self.get_best_vocab_match(transliterated)\n",
    "\n",
    "            result_tokens.append(transliterated)\n",
    "\n",
    "        return \" \".join(result_tokens)\n",
    "\n",
    "# Build Vocab from the dataset (pure Arabic tokens only)\n",
    "all_text = \" \".join(df[\"text\"].tolist())\n",
    "all_tokens = all_text.split()\n",
    "# Filter: must be Arabic chars only, length > 1\n",
    "arabic_vocab = set(t for t in all_tokens if re.match(r'^[ء-ي]+$', t) and len(t) > 1)\n",
    "\n",
    "print(f\"Built Arabic Vocab: {len(arabic_vocab)} words\")\n",
    "\n",
    "# Initialize CODA-compliant Transliterator\n",
    "transliterator = CODATunisianTransliterator(list(arabic_vocab))\n",
    "\n",
    "# Comprehensive Testing Examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CODA-COMPLIANT TUNISIAN ARABIZI TRANSLITERATION TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_cases = [\n",
    "    # Basic digit mappings\n",
    "    (\"7keya 3la tounes\", \"Story about Tunisia\"),\n",
    "    (\"9rib men dar\", \"Close to home\"),\n",
    "\n",
    "    # Exception lexicon\n",
    "    (\"nchal ça va\", \"Inshallah how are you\"),\n",
    "    (\"md rabbi\", \"Thank God\"),\n",
    "\n",
    "    # Multi-character patterns\n",
    "    (\"chkoun khali\", \"Who is my uncle\"),\n",
    "    (\"dhaw ghali\", \"Light is expensive\"),\n",
    "\n",
    "    # Emphatic context (s->ص, t->ط, d->ض)\n",
    "    (\"saber wa nasser\", \"Saber and Nasser - emphatic context\"),\n",
    "\n",
    "    # CODA markers: + for joining\n",
    "    (\"3al+tawla\", \"On the table - joined with +\"),\n",
    "    (\"fel+dar\", \"In the house - joined with +\"),\n",
    "\n",
    "    # CODA markers: - for splitting\n",
    "    (\"wa9t-el-3achia\", \"Evening time - split with -\"),\n",
    "\n",
    "    # Repeated letters (emphasis)\n",
    "    (\"bniiiiiina\", \"We built - with emphasis\"),\n",
    "    (\"7loooow\", \"Sweet - with elongation\"),\n",
    "\n",
    "    # Mixed content with emoticons\n",
    "    (\"barcha behi :) merci\", \"Very good - with emoticon and French\"),\n",
    "\n",
    "    # Complex sentence\n",
    "    (\"nchal rabi y7afdh tounes 9wiya\", \"Inshallah God protects strong Tunisia\"),\n",
    "\n",
    "    # Common Tunisian expressions\n",
    "    (\"chbeik chwaya barcha\", \"What's wrong with you a bit a lot\"),\n",
    "    (\"kifech win winek\", \"How where are you\"),\n",
    "]\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"-\" * 80)\n",
    "for arabizi_text, description in test_cases:\n",
    "    result = transliterator.transliterate_text(arabizi_text, use_vocab_matching=False)\n",
    "    print(f\"Input:  {arabizi_text}\")\n",
    "    print(f\"Output: {result}\")\n",
    "    print(f\"Note:   {description}\")\n",
    "    print()\n"
   ],
   "id": "998c790ed449a8c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Arabic Vocab: 34734 words\n",
      "\n",
      "================================================================================\n",
      "CODA-COMPLIANT TUNISIAN ARABIZI TRANSLITERATION TESTS\n",
      "================================================================================\n",
      "\n",
      "Test Results:\n",
      "--------------------------------------------------------------------------------\n",
      "Input:  7keya 3la tounes\n",
      "Output: حكايا علا تونس\n",
      "Note:   Story about Tunisia\n",
      "\n",
      "Input:  9rib men dar\n",
      "Output: قريب مان دار\n",
      "Note:   Close to home\n",
      "\n",
      "Input:  nchal ça va\n",
      "Output: إن شاء الله çا فا\n",
      "Note:   Inshallah how are you\n",
      "\n",
      "Input:  md rabbi\n",
      "Output: الحمد لله راببي\n",
      "Note:   Thank God\n",
      "\n",
      "Input:  chkoun khali\n",
      "Output: شكون خلي\n",
      "Note:   Who is my uncle\n",
      "\n",
      "Input:  dhaw ghali\n",
      "Output: ضو غلي\n",
      "Note:   Light is expensive\n",
      "\n",
      "Input:  saber wa nasser\n",
      "Output: سابار وا ناسسار\n",
      "Note:   Saber and Nasser - emphatic context\n",
      "\n",
      "Input:  3al+tawla\n",
      "Output: علتاولا\n",
      "Note:   On the table - joined with +\n",
      "\n",
      "Input:  fel+dar\n",
      "Output: فالدار\n",
      "Note:   In the house - joined with +\n",
      "\n",
      "Input:  wa9t-el-3achia\n",
      "Output: واقت ال عشا\n",
      "Note:   Evening time - split with -\n",
      "\n",
      "Input:  bniiiiiina\n",
      "Output: بنينا\n",
      "Note:   We built - with emphasis\n",
      "\n",
      "Input:  7loooow\n",
      "Output: حلوو\n",
      "Note:   Sweet - with elongation\n",
      "\n",
      "Input:  barcha behi :) merci\n",
      "Output: برشا باهي # merci\n",
      "Note:   Very good - with emoticon and French\n",
      "\n",
      "Input:  nchal rabi y7afdh tounes 9wiya\n",
      "Output: إن شاء الله رابي يحفذ تونس قوييا\n",
      "Note:   Inshallah God protects strong Tunisia\n",
      "\n",
      "Input:  chbeik chwaya barcha\n",
      "Output: شبيك شوية برشا\n",
      "Note:   What's wrong with you a bit a lot\n",
      "\n",
      "Input:  kifech win winek\n",
      "Output: كيفاش وين وينك\n",
      "Note:   How where are you\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T11:57:32.550883200Z",
     "start_time": "2025-12-23T11:54:54.711659400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================================================\n",
    "# DATABASE SAMPLE TESTING: Apply CODA Translation to Real Dataset\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING ON DATABASE SAMPLE (30 tweets)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select a diverse sample with different patterns\n",
    "sample_indices = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70,\n",
    "                  100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800]\n",
    "test_sample_df = df.iloc[sample_indices[:30]].copy()\n",
    "\n",
    "# Apply CODA transliteration without vocab matching first (to see pure dictionary translation)\n",
    "print(\"\\nTranslating sample tweets...\")\n",
    "test_sample_df['coda_translation'] = test_sample_df['text'].apply(\n",
    "    lambda x: transliterator.transliterate_text(str(x), use_vocab_matching=False)\n",
    ")\n",
    "\n",
    "# Display results with comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSLATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in test_sample_df.head(15).iterrows():\n",
    "    print(f\"\\n[Row {idx}] Label: {row['label']}\")\n",
    "    print(f\"Original Tweet: {row['Tweet'][:80]}...\")\n",
    "    print(f\"Preprocessed:   {row['text'][:80]}...\")\n",
    "    print(f\"CODA Arabic:    {row['coda_translation'][:80]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Save detailed results to CSV\n",
    "output_file = \"coda_translation_sample_results.csv\"\n",
    "test_sample_df[['Tweet', 'text', 'coda_translation', 'label']].to_csv(\n",
    "    output_file, index=False, encoding='utf-8-sig'\n",
    ")\n",
    "print(f\"\\n✓ Detailed results saved to: {output_file}\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSLATION STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total tweets tested: {len(test_sample_df)}\")\n",
    "print(f\"Successfully translated: {test_sample_df['coda_translation'].notna().sum()}\")\n",
    "\n",
    "# Show distribution by label\n",
    "print(\"\\nDistribution by label:\")\n",
    "print(test_sample_df['label'].value_counts())\n",
    "\n",
    "# Analyze translation patterns\n",
    "def analyze_translation(original, translated):\n",
    "    \"\"\"Simple analysis of what changed\"\"\"\n",
    "    has_arabic_numbers = bool(re.search(r'[0-9]', original))\n",
    "    has_latin = bool(re.search(r'[a-zA-Z]', original))\n",
    "    has_arabic_script = bool(re.search(r'[ء-ي]', translated))\n",
    "    return {\n",
    "        'had_numbers': has_arabic_numbers,\n",
    "        'had_latin': has_latin,\n",
    "        'now_arabic': has_arabic_script\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE COMPARISONS (First 10)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_results = []\n",
    "for idx, row in test_sample_df.head(10).iterrows():\n",
    "    analysis = analyze_translation(row['text'], row['coda_translation'])\n",
    "    comparison_results.append({\n",
    "        'index': idx,\n",
    "        'original': row['text'][:50],\n",
    "        'translated': row['coda_translation'][:50],\n",
    "        **analysis\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "# Now apply to full dataset with vocab matching\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Applying CODA transliteration to FULL dataset with vocabulary matching...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df[\"text_coda\"] = df[\"text\"].apply(\n",
    "    lambda x: transliterator.transliterate_text(str(x), use_vocab_matching=True)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Translation complete! Processed {len(df)} texts.\")\n",
    "print(\"\\nPreview of full dataset results:\")\n",
    "preview_df = df[[\"Tweet\", \"text\", \"text_coda\",\"translated_dict\", \"label\"]].head(10)\n",
    "print(preview_df.to_string())\n"
   ],
   "id": "cfaa7aaf7f8e5d8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING ON DATABASE SAMPLE (30 tweets)\n",
      "================================================================================\n",
      "\n",
      "Translating sample tweets...\n",
      "\n",
      "================================================================================\n",
      "TRANSLATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "[Row 0] Label: 1\n",
      "Original Tweet: Nn mouch 7louwa faza...\n",
      "Preprocessed:   nn mouch 7louwa faza...\n",
      "CODA Arabic:    نن موش حلووا فازا...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 5] Label: 1\n",
      "Original Tweet: b rjoulia stoufa to7t men3ini ..........\n",
      "Preprocessed:   b rjoulia stoufa to7t men3ini ....\n",
      "CODA Arabic:    ب رجوليا ستوفا توحت مانعني ....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 10] Label: 1\n",
      "Original Tweet: كلاب أولاد كلاب...\n",
      "Preprocessed:   كلاب اولاد كلاب...\n",
      "CODA Arabic:    كلاب اولاد كلاب...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 15] Label: 1\n",
      "Original Tweet: المسلسل لي تحكو عليه والله سمعت بيه كان من تعليقاتكم ههههه كفاش تتفرجو عليه و كف...\n",
      "Preprocessed:   المسلسل لي تحكو عليه والله سمعت بيه كان من تعليقاتكم هه كفاش تتفرجو عليه و كفاش ...\n",
      "CODA Arabic:    المسلسل لي تحكو عليه والله سمعت بيه كان من تعليقاتكم هه كفاش تتفرجو عليه و كفاش ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 20] Label: 1\n",
      "Original Tweet: في رمضان يقولو يغيبو الشواطن اما تحضر قناة الحمار التونسي...\n",
      "Preprocessed:   في رمضان يقولو يغيبو الشواطن اما تحضر قناة الحمار التونسي...\n",
      "CODA Arabic:    في رمضان يقولو يغيبو الشواطن اما تحضر قناة الحمار التونسي...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 25] Label: 1\n",
      "Original Tweet: مريض نفسي والناس لكل تكرهك يا مقداد يا تافه...\n",
      "Preprocessed:   مريض نفسي والناس لكل تكرهك يا مقداد يا تافه...\n",
      "CODA Arabic:    مريض نفسي والناس لكل تكرهك يا مقداد يا تافه...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 30] Label: 1\n",
      "Original Tweet: مثقف بودورو . ممثل فاشل راك مقزز\"...\n",
      "Preprocessed:   مثقف بودورو . ممثل فاشل راك مقزز\"...\n",
      "CODA Arabic:    مثقف بودورو . ممثل فاشل راك مقزز\"...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 35] Label: 1\n",
      "Original Tweet: tfuuuuuuuuuuuuh puuuuuute...\n",
      "Preprocessed:   tfuuh puute...\n",
      "CODA Arabic:    تفوه بوتا...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 40] Label: 1\n",
      "Original Tweet: Cha3b tafeh...\n",
      "Preprocessed:   cha3b tafeh...\n",
      "CODA Arabic:    شعب تافاه...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 45] Label: 1\n",
      "Original Tweet: mala maset i7eb yestabandi besiffff...\n",
      "Preprocessed:   mala maset i7eb yestabandi besiff...\n",
      "CODA Arabic:    مالا ماسات يحب ياستاباندي باسيفف...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 50] Label: 1\n",
      "Original Tweet: تافه كي عاتو...ماهوش متربي...\n",
      "Preprocessed:   تافه كي عاتو.ماهوش متربي...\n",
      "CODA Arabic:    تافه كي عاتو.ماهوش متربي...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 55] Label: 1\n",
      "Original Tweet: yezzi bla dharbane lougha rana t3ebna rak 7aj ya 7aj hhhh...\n",
      "Preprocessed:   yezzi bla dharbane lougha rana t3ebna rak 7aj ya 7aj hh...\n",
      "CODA Arabic:    يزي بلا ضربانا لوغ رانا تعبنا راك حج يا حج هه...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 60] Label: 1\n",
      "Original Tweet: Brabi yeziiiii mlkedheb...\n",
      "Preprocessed:   brabi yezii mlkedheb...\n",
      "CODA Arabic:    برابي يازي ملكاضب...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 65] Label: 1\n",
      "Original Tweet: oooooooh naoufel mamstou...\n",
      "Preprocessed:   ooh naoufel mamstou...\n",
      "CODA Arabic:    وه ناوفال مامستو...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Row 70] Label: 1\n",
      "Original Tweet: Billehi hak elkalb nour chiba tfouh 3lik w 3la ekalb likbir neji jalloul...\n",
      "Preprocessed:   billehi hak elkalb nour chiba tfouh 3lik w 3la ekalb likbir neji jalloul...\n",
      "CODA Arabic:    بيللاهي هاك الكالب نور شبا تفوه عليك و علا اكالب ليكبير ناجي جاللول...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ Detailed results saved to: coda_translation_sample_results.csv\n",
      "\n",
      "================================================================================\n",
      "TRANSLATION STATISTICS\n",
      "================================================================================\n",
      "Total tweets tested: 30\n",
      "Successfully translated: 30\n",
      "\n",
      "Distribution by label:\n",
      "label\n",
      "1    30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "SAMPLE COMPARISONS (First 10)\n",
      "================================================================================\n",
      "   index                                            original                                          translated  had_numbers  had_latin  now_arabic\n",
      "0      0                                nn mouch 7louwa faza                                   نن موش حلووا فازا         True       True        True\n",
      "1      5                     b rjoulia stoufa to7t men3ini .                        ب رجوليا ستوفا توحت مانعني .         True       True        True\n",
      "2     10                                     كلاب اولاد كلاب                                     كلاب اولاد كلاب        False      False        True\n",
      "3     15  المسلسل لي تحكو عليه والله سمعت بيه كان من تعليقات  المسلسل لي تحكو عليه والله سمعت بيه كان من تعليقات        False      False        True\n",
      "4     20  في رمضان يقولو يغيبو الشواطن اما تحضر قناة الحمار   في رمضان يقولو يغيبو الشواطن اما تحضر قناة الحمار         False      False        True\n",
      "5     25         مريض نفسي والناس لكل تكرهك يا مقداد يا تافه         مريض نفسي والناس لكل تكرهك يا مقداد يا تافه        False      False        True\n",
      "6     30                   مثقف بودورو . ممثل فاشل راك مقزز\"                   مثقف بودورو . ممثل فاشل راك مقزز\"        False      False        True\n",
      "7     35                                         tfuuh puute                                           تفوه بوتا        False       True        True\n",
      "8     40                                         cha3b tafeh                                           شعب تافاه         True       True        True\n",
      "9     45                   mala maset i7eb yestabandi besiff                    مالا ماسات يحب ياستاباندي باسيفف         True       True        True\n",
      "\n",
      "================================================================================\n",
      "Applying CODA transliteration to FULL dataset with vocabulary matching...\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T11:59:52.025414Z",
     "start_time": "2025-12-23T11:59:20.279711100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Any, Dict\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
    "\n",
    "# token length stats on *CODA-transliterated* text\n",
    "lengths = [len(tokenizer(t, add_special_tokens=True).input_ids) for t in df[\"translated_dict\"]]\n",
    "percentiles = np.percentile(lengths, [50, 75, 90, 95, 99])\n",
    "print(f\"Token Length Percentiles (50, 75, 90, 95, 99): {percentiles}\")\n"
   ],
   "id": "55ad96ebbc46cbc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Length Percentiles (50, 75, 90, 95, 99): [ 9. 15. 26. 37. 79.]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:01:31.539524Z",
     "start_time": "2025-12-23T12:01:31.458621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Show tokenization as tokens (strings), not only IDs\n",
    "sample = df.loc[0, \"translated_dict\"]\n",
    "\n",
    "tokens = tokenizer.tokenize(sample)\n",
    "encoded: Dict[str, Any] = tokenizer(sample, add_special_tokens=True)\n",
    "\n",
    "tokens_with_special = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"])\n",
    "\n",
    "print(\"TEXT:\", sample)\n",
    "print(\"\\nTOKENS (no special tokens):\")\n",
    "print(tokens)\n",
    "print(\"\\nTOKENS (with special tokens):\")\n",
    "print(tokens_with_special)\n"
   ],
   "id": "c24ddece8c5f74bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: نن موش حلووا فازا\n",
      "\n",
      "TOKENS (no special tokens):\n",
      "['نن', 'موش', 'حلوو', '##ا', 'فاز', '##ا']\n",
      "\n",
      "TOKENS (with special tokens):\n",
      "['[CLS]', 'نن', 'موش', 'حلوو', '##ا', 'فاز', '##ا', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T12:37:19.682933Z",
     "start_time": "2025-12-23T12:37:13.540998800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================================================\n",
    "# PHASE 2: COMPLETE TOKENIZATION PIPELINE + CLASS IMBALANCE HANDLING\n",
    "# ========================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2: TOKENIZATION & DATA PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================================================\n",
    "# Step 1: Analyze Class Imbalance\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 1: CLASS IMBALANCE ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Current distribution (labels are already integers 0 and 1)\n",
    "label_counts = df['label'].value_counts()\n",
    "print(\"\\nOriginal Distribution:\")\n",
    "print(label_counts)\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Positive (0): {(df['label']==0).sum()} ({(df['label']==0).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Negative (1): {(df['label']==1).sum()} ({(df['label']==1).sum()/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Label mapping for reference\n",
    "label_map = {0: 'positive', 1: 'negative'}\n",
    "print(f\"\\nLabel mapping: {label_map}\")\n",
    "print(f\"Imbalance ratio: {(df['label']==0).sum() / (df['label']==1).sum():.2f}:1\")\n",
    "\n",
    "# Compute class weights for training\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(df['label']),\n",
    "    y=df['label']\n",
    ")\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"\\n✓ Class weights computed for balanced training:\")\n",
    "print(f\"  Class 0 (positive): {class_weight_dict[0]:.4f}\")\n",
    "print(f\"  Class 1 (negative): {class_weight_dict[1]:.4f}\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 2: Train/Test Split with Stratification\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 2: STRATIFIED TRAIN/TEST SPLIT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Use translated_dict as input (best translation)\n",
    "X = df['translated_dict'].values\n",
    "y = df['label'].values  # Labels are already integers (0, 1)\n",
    "\n",
    "# Stratified split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train)} samples\")\n",
    "print(f\"  Positive (0): {(y_train==0).sum()} ({(y_train==0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Negative (1): {(y_train==1).sum()} ({(y_train==1).sum()/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set: {len(X_test)} samples\")\n",
    "print(f\"  Positive (0): {(y_test==0).sum()} ({(y_test==0).sum()/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Negative (1): {(y_test==1).sum()} ({(y_test==1).sum()/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 3: Initialize MARBERT Tokenizer\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 3: INITIALIZE MARBERT TOKENIZER\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "MODEL_NAME = \"UBC-NLP/MARBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"✓ Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Model max length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# Determine optimal max_length based on token length analysis\n",
    "print(\"\\n✓ Analyzing token lengths...\")\n",
    "sample_texts = X_train[:1000]  # Sample for speed\n",
    "sample_lengths = [len(tokenizer(t, add_special_tokens=True).input_ids) for t in sample_texts]\n",
    "percentiles = np.percentile(sample_lengths, [50, 75, 90, 95, 99])\n",
    "\n",
    "print(f\"Token length percentiles (50, 75, 90, 95, 99): {percentiles}\")\n",
    "print(f\"Recommended max_length: 64 (covers ~90% of data)\")\n",
    "\n",
    "MAX_LENGTH = 64\n",
    "\n",
    "# ========================================================================\n",
    "# Step 4: Create PyTorch Dataset\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 4: CREATE PYTORCH DATASET\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "class TunisianSentimentDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for Tunisian dialect sentiment analysis\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TunisianSentimentDataset(X_train, y_train, tokenizer, MAX_LENGTH)\n",
    "test_dataset = TunisianSentimentDataset(X_test, y_test, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"✓ Train dataset created: {len(train_dataset)} samples\")\n",
    "print(f\"✓ Test dataset created: {len(test_dataset)} samples\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 5: Tokenize Sample and Inspect\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 5: TOKENIZATION INSPECTION (Sample)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get a sample from training data\n",
    "sample_idx = 3\n",
    "sample_data = train_dataset[sample_idx]\n",
    "\n",
    "print(f\"\\nOriginal text: {X_train[sample_idx][:100]}...\")\n",
    "print(f\"Label: {y_train[sample_idx]} ({'positive' if y_train[sample_idx]==0 else 'negative'})\")\n",
    "\n",
    "print(f\"\\nTokenized output:\")\n",
    "print(f\"  input_ids shape: {sample_data['input_ids'].shape}\")\n",
    "print(f\"  attention_mask shape: {sample_data['attention_mask'].shape}\")\n",
    "print(f\"  label: {sample_data['label'].item()}\")\n",
    "\n",
    "# Convert IDs to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(sample_data['input_ids'])\n",
    "print(f\"\\nTokens (first 20): {tokens[:20]}\")\n",
    "\n",
    "# Count non-padding tokens\n",
    "non_padding = sample_data['attention_mask'].sum().item()\n",
    "print(f\"Non-padding tokens: {non_padding}/{MAX_LENGTH}\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 6: Create DataLoaders with Weighted Sampling\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 6: CREATE DATALOADERS (with weighted sampling)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create weighted sampler for training to handle class imbalance\n",
    "# Calculate weights for each sample based on its class\n",
    "sample_weights = [class_weight_dict[label] for label in y_train]\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Training DataLoader with weighted sampler\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=0  # Set to 0 for Windows compatibility\n",
    ")\n",
    "\n",
    "# Test DataLoader (no sampling needed)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"✓ Train DataLoader: {len(train_loader)} batches (batch_size={BATCH_SIZE})\")\n",
    "print(f\"✓ Test DataLoader: {len(test_loader)} batches (batch_size={BATCH_SIZE})\")\n",
    "print(f\"✓ Weighted sampling enabled for class balance\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 7: Verify Batch Structure\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 7: VERIFY BATCH STRUCTURE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nBatch structure:\")\n",
    "print(f\"  input_ids: {batch['input_ids'].shape} (batch_size, max_length)\")\n",
    "print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
    "print(f\"  labels: {batch['label'].shape} (batch_size,)\")\n",
    "\n",
    "print(f\"\\nBatch label distribution:\")\n",
    "unique, counts = torch.unique(batch['label'], return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    label_name = 'positive' if label == 0 else 'negative'\n",
    "    print(f\"  {label_name}: {count.item()}/{BATCH_SIZE}\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 8: Full Dataset Tokenization Summary\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKENIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = {\n",
    "    \"Model\": MODEL_NAME,\n",
    "    \"Tokenizer vocab size\": tokenizer.vocab_size,\n",
    "    \"Max length\": MAX_LENGTH,\n",
    "    \"Total samples\": len(df),\n",
    "    \"Train samples\": len(train_dataset),\n",
    "    \"Test samples\": len(test_dataset),\n",
    "    \"Batch size\": BATCH_SIZE,\n",
    "    \"Train batches\": len(train_loader),\n",
    "    \"Test batches\": len(test_loader),\n",
    "    \"Class 0 weight\": f\"{class_weight_dict[0]:.4f}\",\n",
    "    \"Class 1 weight\": f\"{class_weight_dict[1]:.4f}\",\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ PHASE 2 COMPLETE: Ready for model training!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save tokenization config for backend\n",
    "tokenization_config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'class_weights': class_weight_dict,\n",
    "    'label_map': label_map,\n",
    "    'train_size': len(train_dataset),\n",
    "    'test_size': len(test_dataset),\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Configuration saved for backend integration\")\n"
   ],
   "id": "c153669c5c7f6223",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: TOKENIZATION & DATA PREPARATION\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 1: CLASS IMBALANCE ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Original Distribution:\n",
      "label\n",
      "0    35239\n",
      "1    12556\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total samples: 47795\n",
      "Positive (0): 35239 (73.73%)\n",
      "Negative (1): 12556 (26.27%)\n",
      "\n",
      "Label mapping: {0: 'positive', 1: 'negative'}\n",
      "Imbalance ratio: 2.81:1\n",
      "\n",
      "✓ Class weights computed for balanced training:\n",
      "  Class 0 (positive): 0.6782\n",
      "  Class 1 (negative): 1.9033\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 2: STRATIFIED TRAIN/TEST SPLIT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Train set: 38236 samples\n",
      "  Positive (0): 28191 (73.7%)\n",
      "  Negative (1): 10045 (26.3%)\n",
      "\n",
      "Test set: 9559 samples\n",
      "  Positive (0): 7048 (73.7%)\n",
      "  Negative (1): 2511 (26.3%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 3: INITIALIZE MARBERT TOKENIZER\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Tokenizer loaded: UBC-NLP/MARBERT\n",
      "  Vocab size: 100000\n",
      "  Model max length: 1000000000000000019884624838656\n",
      "\n",
      "✓ Analyzing token lengths...\n",
      "Token length percentiles (50, 75, 90, 95, 99): [ 9.   16.   27.   36.   83.11]\n",
      "Recommended max_length: 64 (covers ~90% of data)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 4: CREATE PYTORCH DATASET\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Train dataset created: 38236 samples\n",
      "✓ Test dataset created: 9559 samples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 5: TOKENIZATION INSPECTION (Sample)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Original text: تعجبني مراة مضخمه غير ضحكتها صفرء !...\n",
      "Label: 0 (positive)\n",
      "\n",
      "Tokenized output:\n",
      "  input_ids shape: torch.Size([64])\n",
      "  attention_mask shape: torch.Size([64])\n",
      "  label: 0\n",
      "\n",
      "Tokens (first 20): ['[CLS]', 'تعجبني', 'مراة', 'مض', '##خمه', 'غير', 'ضحكتها', 'صفر', '##ء', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Non-padding tokens: 11/64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6: CREATE DATALOADERS (with weighted sampling)\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Train DataLoader: 2390 batches (batch_size=16)\n",
      "✓ Test DataLoader: 598 batches (batch_size=16)\n",
      "✓ Weighted sampling enabled for class balance\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 7: VERIFY BATCH STRUCTURE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Batch structure:\n",
      "  input_ids: torch.Size([16, 64]) (batch_size, max_length)\n",
      "  attention_mask: torch.Size([16, 64])\n",
      "  labels: torch.Size([16]) (batch_size,)\n",
      "\n",
      "Batch label distribution:\n",
      "  positive: 9/16\n",
      "  negative: 7/16\n",
      "\n",
      "================================================================================\n",
      "TOKENIZATION SUMMARY\n",
      "================================================================================\n",
      "Model                    : UBC-NLP/MARBERT\n",
      "Tokenizer vocab size     : 100000\n",
      "Max length               : 64\n",
      "Total samples            : 47795\n",
      "Train samples            : 38236\n",
      "Test samples             : 9559\n",
      "Batch size               : 16\n",
      "Train batches            : 2390\n",
      "Test batches             : 598\n",
      "Class 0 weight           : 0.6782\n",
      "Class 1 weight           : 1.9033\n",
      "\n",
      "================================================================================\n",
      "✓ PHASE 2 COMPLETE: Ready for model training!\n",
      "================================================================================\n",
      "\n",
      "✓ Configuration saved for backend integration\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "92878fcc43860838"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
