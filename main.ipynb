{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-14T14:17:23.855755Z",
     "start_time": "2025-12-14T14:17:18.677144700Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset(\"arbml/Tunisian_Dialect_Corpus\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df.to_csv('tunisian_dialect_corpus.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T14:27:40.093645900Z",
     "start_time": "2025-12-14T14:27:40.034461700Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "6bc5b5f7dd744be8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   Tweet  label\n",
       "0                                   Nn mouch 7louwa faza      1\n",
       "1                                mabladkom 3bed tfouuuuh      1\n",
       "2      تفووووووه عليك و علا والديك على عايلتك و على ا...      1\n",
       "3                                  لا يليق بهذا البرنامج      1\n",
       "4                                                  رهدان      1\n",
       "...                                                  ...    ...\n",
       "49884                                        الله يستر ن      0\n",
       "49885                                        الله يستر ن      0\n",
       "49886  ربي اكون فى عونكم بالحق ربي ابقي الستر على تون...      0\n",
       "49887                                                         0\n",
       "49888                                      ربي يلطف بينا      0\n",
       "\n",
       "[49889 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nn mouch 7louwa faza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mabladkom 3bed tfouuuuh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تفووووووه عليك و علا والديك على عايلتك و على ا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لا يليق بهذا البرنامج</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>رهدان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49884</th>\n",
       "      <td>الله يستر ن</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49885</th>\n",
       "      <td>الله يستر ن</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49886</th>\n",
       "      <td>ربي اكون فى عونكم بالحق ربي ابقي الستر على تون...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49887</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49888</th>\n",
       "      <td>ربي يلطف بينا</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49889 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:15:26.172522900Z",
     "start_time": "2025-12-14T15:15:16.135709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
    "\n",
    "lengths = []\n",
    "for text in df[\"Tweet\"]:\n",
    "    lengths.append(len(tokenizer(text).input_ids))\n",
    "\n",
    "np.percentile(lengths, [50, 75, 90, 95, 99])\n"
   ],
   "id": "552ee1ec5943a7d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 17., 30., 42., 89.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:22:35.546571900Z",
     "start_time": "2025-12-14T15:22:35.463589500Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(lengths))",
   "id": "9418acca879399ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49889\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:25:14.303026900Z",
     "start_time": "2025-12-14T15:25:14.231442300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for text in df[\"Tweet\"][0:10]:\n",
    "    print(text)\n",
    "    print(tokenizer(text))\n",
    "    print()"
   ],
   "id": "8e6de518502a2e01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nn mouch 7louwa faza\n",
      "{'input_ids': [2, 53, 1057, 88519, 6432, 86608, 3107, 8041, 38796, 34104, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "mabladkom 3bed tfouuuuh\n",
      "{'input_ids': [2, 22570, 35896, 2664, 59403, 1113, 21, 35136, 1055, 59, 1173, 3107, 1058, 1058, 59181, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "تفووووووه عليك و علا والديك على عايلتك و على اصلك و على فصلك و على الي يعرفك و باش يعرفك و على الي مصوحبك و مجاورك و على معارفك و على الارض الي هازتك و على تونس الي قابلتك تفووووووووووووووووووه\n",
      "{'input_ids': [2, 47391, 5005, 18629, 2373, 144, 3407, 13623, 1977, 34741, 144, 1977, 23719, 144, 1977, 8576, 1012, 144, 1977, 2019, 20582, 144, 7157, 20582, 144, 1977, 2019, 2490, 2392, 2235, 144, 33929, 4507, 144, 1977, 57069, 1012, 144, 1977, 2590, 2019, 22882, 2035, 144, 1977, 7682, 2019, 82130, 47391, 5005, 5005, 5005, 5005, 5005, 5005, 5005, 18629, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "لا يليق بهذا البرنامج\n",
      "{'input_ids': [2, 1956, 8848, 5690, 8415, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "رهدان\n",
      "{'input_ids': [2, 5693, 27644, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "\n",
      "b rjoulia stoufa to7t men3ini .......\n",
      "{'input_ids': [2, 41, 57, 32123, 7948, 3302, 4946, 3107, 20548, 5436, 1087, 1050, 40640, 1052, 67975, 16, 16, 16, 16, 16, 16, 16, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "قردة بهيمة حمارة بغلة......... والمزيد الخ ابشع عفانة شفتها في حياتي لازم يقام عليها الحد\n",
      "{'input_ids': [2, 25839, 1046, 73258, 98390, 64599, 1046, 16, 16, 16, 16, 16, 16, 16, 16, 16, 52729, 2057, 18512, 40902, 1046, 15083, 1947, 3258, 3085, 36860, 2839, 4649, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "tozzzzzzzzzzzzzzzzzzzz fi 9anet el 7iwaer ettounssi 9anet eda3ara\n",
      "{'input_ids': [2, 5436, 42317, 42317, 42317, 42317, 42317, 42317, 42317, 42317, 42317, 42317, 49829, 27, 57060, 1050, 10294, 25, 1108, 8041, 1930, 31082, 49953, 7971, 36736, 1108, 27, 57060, 1050, 28477, 1066, 78457, 2455, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "شنوى هذا شنية هالمسخرة ؟؟؟؟؟؟؟؟؟؟؟؟\n",
      "{'input_ids': [2, 4942, 1051, 2158, 48237, 1046, 44027, 5939, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "مرا مخيب راسك يا هايشة\n",
      "{'input_ids': [2, 3814, 94955, 7449, 2023, 5241, 3624, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:26:47.131659600Z",
     "start_time": "2025-12-14T15:26:46.987704700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for text in df[\"Tweet\"][0:10]:\n",
    "    print(f\"Original text: {text}\")\n",
    "\n",
    "    # Get the tokenization result\n",
    "    tokens = tokenizer(text)\n",
    "\n",
    "    # Decode individual tokens to see how text is split\n",
    "    token_texts = tokenizer.convert_ids_to_tokens(tokens['input_ids'])\n",
    "\n",
    "    print(f\"Tokenized: {token_texts}\")\n",
    "    print(f\"Number of tokens: {len(tokens['input_ids'])}\")\n",
    "    print(f\"Token IDs: {tokens['input_ids']}\")\n",
    "    print(\"-\" * 50)"
   ],
   "id": "7e5b50dc1661e157",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Nn mouch 7louwa faza\n",
      "Tokenized: ['[CLS]', 'n', '##n', 'mou', '##ch', '7l', '##ou', '##wa', 'fa', '##za', '[SEP]']\n",
      "Number of tokens: 11\n",
      "Token IDs: [2, 53, 1057, 88519, 6432, 86608, 3107, 8041, 38796, 34104, 3]\n",
      "--------------------------------------------------\n",
      "Original text: mabladkom 3bed tfouuuuh\n",
      "Tokenized: ['[CLS]', 'ma', '##bl', '##ad', '##ko', '##m', '3', '##be', '##d', 't', '##f', '##ou', '##u', '##u', '##uh', '[SEP]']\n",
      "Number of tokens: 16\n",
      "Token IDs: [2, 22570, 35896, 2664, 59403, 1113, 21, 35136, 1055, 59, 1173, 3107, 1058, 1058, 59181, 3]\n",
      "--------------------------------------------------\n",
      "Original text: تفووووووه عليك و علا والديك على عايلتك و على اصلك و على فصلك و على الي يعرفك و باش يعرفك و على الي مصوحبك و مجاورك و على معارفك و على الارض الي هازتك و على تونس الي قابلتك تفووووووووووووووووووه\n",
      "Tokenized: ['[CLS]', 'تفوو', '##وو', '##ووه', 'عليك', 'و', 'علا', 'والديك', 'على', 'عايلتك', 'و', 'على', 'اصلك', 'و', 'على', 'فصل', '##ك', 'و', 'على', 'الي', 'يعرفك', 'و', 'باش', 'يعرفك', 'و', 'على', 'الي', 'مص', '##وح', '##بك', 'و', 'مجا', '##ورك', 'و', 'على', 'معارف', '##ك', 'و', 'على', 'الارض', 'الي', 'هاز', '##تك', 'و', 'على', 'تونس', 'الي', 'قابلتك', 'تفوو', '##وو', '##وو', '##وو', '##وو', '##وو', '##وو', '##وو', '##ووه', '[SEP]']\n",
      "Number of tokens: 58\n",
      "Token IDs: [2, 47391, 5005, 18629, 2373, 144, 3407, 13623, 1977, 34741, 144, 1977, 23719, 144, 1977, 8576, 1012, 144, 1977, 2019, 20582, 144, 7157, 20582, 144, 1977, 2019, 2490, 2392, 2235, 144, 33929, 4507, 144, 1977, 57069, 1012, 144, 1977, 2590, 2019, 22882, 2035, 144, 1977, 7682, 2019, 82130, 47391, 5005, 5005, 5005, 5005, 5005, 5005, 5005, 18629, 3]\n",
      "--------------------------------------------------\n",
      "Original text: لا يليق بهذا البرنامج\n",
      "Tokenized: ['[CLS]', 'لا', 'يليق', 'بهذا', 'البرنامج', '[SEP]']\n",
      "Number of tokens: 6\n",
      "Token IDs: [2, 1956, 8848, 5690, 8415, 3]\n",
      "--------------------------------------------------\n",
      "Original text: رهدان\n",
      "Tokenized: ['[CLS]', 'ره', '##دان', '[SEP]']\n",
      "Number of tokens: 4\n",
      "Token IDs: [2, 5693, 27644, 3]\n",
      "--------------------------------------------------\n",
      "Original text: b rjoulia stoufa to7t men3ini .......\n",
      "Tokenized: ['[CLS]', 'b', 'r', '##jo', '##ul', '##ia', 'st', '##ou', '##fa', 'to', '##7', '##t', 'men', '##3', '##ini', '.', '.', '.', '.', '.', '.', '.', '[SEP]']\n",
      "Number of tokens: 23\n",
      "Token IDs: [2, 41, 57, 32123, 7948, 3302, 4946, 3107, 20548, 5436, 1087, 1050, 40640, 1052, 67975, 16, 16, 16, 16, 16, 16, 16, 3]\n",
      "--------------------------------------------------\n",
      "Original text: قردة بهيمة حمارة بغلة......... والمزيد الخ ابشع عفانة شفتها في حياتي لازم يقام عليها الحد\n",
      "Tokenized: ['[CLS]', 'قرد', '##ة', 'بهيمة', 'حمارة', 'بغل', '##ة', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'والمزيد', 'الخ', 'ابشع', 'عفان', '##ة', 'شفتها', 'في', 'حياتي', 'لازم', 'يقام', 'عليها', 'الحد', '[SEP]']\n",
      "Number of tokens: 29\n",
      "Token IDs: [2, 25839, 1046, 73258, 98390, 64599, 1046, 16, 16, 16, 16, 16, 16, 16, 16, 16, 52729, 2057, 18512, 40902, 1046, 15083, 1947, 3258, 3085, 36860, 2839, 4649, 3]\n",
      "--------------------------------------------------\n",
      "Original text: tozzzzzzzzzzzzzzzzzzzz fi 9anet el 7iwaer ettounssi 9anet eda3ara\n",
      "Tokenized: ['[CLS]', 'to', '##zz', '##zz', '##zz', '##zz', '##zz', '##zz', '##zz', '##zz', '##zz', '##zz', 'fi', '9', '##ane', '##t', 'el', '7', '##i', '##wa', '##er', 'et', '##to', '##un', '##ss', '##i', '9', '##ane', '##t', 'ed', '##a', '##3a', '##ra', '[SEP]']\n",
      "Number of tokens: 34\n",
      "Token IDs: [2, 5436, 42317, 42317, 42317, 42317, 42317, 42317, 42317, 42317, 42317, 42317, 49829, 27, 57060, 1050, 10294, 25, 1108, 8041, 1930, 31082, 49953, 7971, 36736, 1108, 27, 57060, 1050, 28477, 1066, 78457, 2455, 3]\n",
      "--------------------------------------------------\n",
      "Original text: شنوى هذا شنية هالمسخرة ؟؟؟؟؟؟؟؟؟؟؟؟\n",
      "Tokenized: ['[CLS]', 'شنو', '##ى', 'هذا', 'شني', '##ة', 'هالمس', '##خرة', '؟', '؟', '؟', '؟', '؟', '؟', '؟', '؟', '؟', '؟', '؟', '؟', '[SEP]']\n",
      "Number of tokens: 21\n",
      "Token IDs: [2, 4942, 1051, 2158, 48237, 1046, 44027, 5939, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 3]\n",
      "--------------------------------------------------\n",
      "Original text: مرا مخيب راسك يا هايشة\n",
      "Tokenized: ['[CLS]', 'مرا', 'مخيب', 'راسك', 'يا', 'هاي', '##شة', '[SEP]']\n",
      "Number of tokens: 8\n",
      "Token IDs: [2, 3814, 94955, 7449, 2023, 5241, 3624, 3]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a0471c437be9676"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
